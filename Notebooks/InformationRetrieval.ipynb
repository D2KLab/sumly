{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Information Retrieval"
      ],
      "metadata": {
        "id": "u8oHsWfMZzJU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC-oTJ4WdPZV"
      },
      "outputs": [],
      "source": [
        "# Install the necessary libraries\n",
        "!pip3 install biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgfNQZbNJAb8"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "from Bio import Entrez\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the articles directories\n",
        "!mkdir -p /content/articles"
      ],
      "metadata": {
        "id": "5rPwKh8Qm5qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVMjuaV5JK-V"
      },
      "outputs": [],
      "source": [
        "# Utilised functions\n",
        "def search(query, max_articles, start_date, end_date, email):\n",
        "  Entrez.email = email\n",
        "\n",
        "  # Get the PubMed ID of articles according to the query\n",
        "  handle = Entrez.esearch(db='pubmed', sort='relevance', retmax=max_articles, retmode='xml', term=query, mindate=start_date, maxdate=end_date)\n",
        "  results = Entrez.read(handle)\n",
        "\n",
        "  return results\n",
        "\n",
        "def fetch_details(id, email):\n",
        "  Entrez.email = email\n",
        "\n",
        "  # Get the PubMed details of the article\n",
        "  handle = Entrez.efetch(db='pubmed', retmode='xml', id=id)\n",
        "  results = Entrez.read(handle)\n",
        "\n",
        "  return results\n",
        "\n",
        "def get_text(id):\n",
        "\n",
        "  # Check if article can be accessed\n",
        "  url = f'http://www.ncbi.nlm.nih.gov/pmc/articles/pmid/{id}'\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "  except:\n",
        "    return\n",
        "\n",
        "  soup = BeautifulSoup(response.content, features='html.parser')\n",
        "  \n",
        "  # Check if the main contents can be scrapped\n",
        "  div = soup.findAll('p', {'id': re.compile('.*p.*', re.IGNORECASE)})\n",
        "  if len(div) == 0:\n",
        "    return\n",
        "  \n",
        "  # Scrape the main contents of the article\n",
        "  text = ''\n",
        "  for i, tag in enumerate(div):\n",
        "    sentence = ' '.join(string.strip() for string in tag.strings)\n",
        "    text += sentence\n",
        "    if i!=0 and i!=len(div)-1:\n",
        "      text += '\\n\\n'\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ID Retrieval"
      ],
      "metadata": {
        "id": "50x92H1uR8SJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vy5xBrJwJjW8"
      },
      "outputs": [],
      "source": [
        "# Get the PubMed IDs of articles based on the query\n",
        "results = search('chronic airways disease', max_articles=2000, start_date='2010/01/01', end_date='2020/01/01', email='abc@gmail.com')\n",
        "id_list = results['IdList']\n",
        "\n",
        "# Get the title of the PubMed articles\n",
        "titles = {}\n",
        "for id in id_list:\n",
        "  paper = fetch_details(id, email='abc@gmail.com')\n",
        "  titles[id] = paper['PubmedArticle'][0]['MedlineCitation']['Article']['ArticleTitle']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Scrapping"
      ],
      "metadata": {
        "id": "MeaOS3qcoJf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape the text of the PubMed articles\n",
        "articles = []\n",
        "success = []\n",
        "\n",
        "for id in tqdm(id_list):\n",
        "  text = get_text(id)\n",
        "  if text is not None:\n",
        "    articles.append(text)\n",
        "    success.append(id)\n",
        "\n",
        "print(f'\\nNumber of articles scrapped: {len(articles)}\\n')\n",
        "for id in success:\n",
        "    print(f'[{id}]: {titles[id]}')"
      ],
      "metadata": {
        "id": "nAVDIB8EvR9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview a scrapped article\n",
        "print(articles[0])"
      ],
      "metadata": {
        "id": "qGboUNjNScKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save each PubMed article as a text file\n",
        "for i, article in enumerate(articles):\n",
        "  with open('/content/articles' + f'/article_{success[i]}.txt', 'w') as text_file:\n",
        "      text_file.write(article)"
      ],
      "metadata": {
        "id": "5kgNWo2Ag3YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the article directory\n",
        "shutil.make_archive('articles', 'zip', '/content/articles')"
      ],
      "metadata": {
        "id": "gTnZ05AcnSW8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Prototype (IR)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}